<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Jarvis Alpha</title>
    <url>/2024/08/15/Jarvis-Alpha/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>語音助理最廣為人知的應該是 Siri 以及 Alexa 了，而在電影 Iron man 中，有個 AI 叫做 J.A.R.V.I.S，專門幫助 Tony Stark 完成各種任務。雖然電影後面他發生了一些事，導致被另一個叫 Friday 的 AI 取代，有興趣的可以去觀看原作。其實我一直以來都很想有個語音AI，協助我處理各種事。兩年前我曾嘗試用 Open AI 的 ChatGPT API，由於 Open AI 訓練完 GPT3 模型之後，開始從開源逐漸走向收費，而 ChatGPT 的 API 免費版有用量限制，最後我專案還沒完成，卻已經用光所有的額度了。<span id="more"></span>曾經有想過要自己訓練模型，但看著桌上的電腦，又默默地放下這個念頭，現階段訓練 AI 需要的硬體設備對我來說是遙不可及。但經過了兩年的時間，越來越多公司投入 AI 領域而且也「開源」了許多模型，也就是說不需要付費便能使用的 AI 模型。Jarvis 版本的命名採拉丁字母的順序，依照 Alpha, Beta, Gama…. 命名下去。這篇是 Jarvis Alpha 版本，我之後會陸續發部改良後的版本，雖然都叫 Jarvis 但其實使用的工具幾乎不一樣。Jarvis Alpha 版是以 Pyhon 進行編寫，我會將專案放在 <a href="https://github.com/Bakersite/Jarvis-Alpha.git">Jarvis-Alpha</a> 。</p>
<h1 id="Huggingface-的工具"><a href="#Huggingface-的工具" class="headerlink" title="Huggingface 的工具"></a>Huggingface 的工具</h1><h2 id="HuggingChat"><a href="#HuggingChat" class="headerlink" title="HuggingChat"></a>HuggingChat</h2><p>這邊要介紹 <a href="https://huggingface.co/">Huggingface</a>，這是一個存放很多開源模型的平台，他除了有模型以外，也提供深度學習框架，有興趣的可以去探索一下，這邊會著墨在他底下一個叫 <a href="https://huggingface.co/chat/">HuggingChat</a> 的生成式 AI 服務。由於在 Alpha 版中尚未用到使用 Huggingface 上的模型，因此有關模型的取用會留到 Beta 版在說明。</p>
<p>HuggingChat 類似於大家熟悉的 ChatGPT ，比較不一樣在 HuggingChat 有許多種模型可以選擇。HuggingChat 也可以自定義 AI 的目的，像是以資深的軟體工程師來修正程式碼，可以在建立對話時便設定好，不需要每次都發送指定 AI 角色的 prompt。</p>
<p>實作時會需要 Huggingface 的帳號，若要實作，可以先申請。</p>
<p>P.S. 此文章發布時有一款模型 Llama3.1，由 Meta 訓練的開源模型，「據說」可以匹敵 Open AI 的 gpt-4o mini，各位可以玩玩看。</p>
<h2 id="Hugging-Chat-API"><a href="#Hugging-Chat-API" class="headerlink" title="Hugging Chat API"></a>Hugging Chat API</h2><p><a href="https://github.com/Soulter/hugging-chat-api">Hugging-Chat-API</a>它是一個大佬寫的，並不是 Huggingface 官方的 API，在 Alpha 版中我會利用它來連接 Hugging Chat 當作 Jarvis 的大腦，若有要實作的請先下載，建議參閱網站的說明文件，下面的安裝參考會比較簡潔。說明文件有詳細的指引，大家也可以試試用他的說明指引，自己做一個語音 AI。</p>
<h1 id="實作"><a href="#實作" class="headerlink" title="實作"></a>實作</h1><h2 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h2><pre class="mermaid">flowchart LR
A(Speech to Text) 
B(Hugging Chat API) 
C[(Hugging Chat server)]
D(Text to Speech)
A --> B --> C --> D --> A</pre>

<p>語音 AI 主要可以拆成三個步驟。</p>
<ol>
<li>語音辨識成文字 </li>
<li>由生成式 AI 當作大腦進行理解 </li>
<li>將生成的文字合成語音並播放</li>
</ol>
<h2 id="語音辨識"><a href="#語音辨識" class="headerlink" title="語音辨識"></a>語音辨識</h2><p>在這裡我們使用 Python 一個叫 <a href="https://pypi.org/project/SpeechRecognition/">Speech Recognition</a> 的工具，利用這工具將語音轉換成文字。以下是簡易的安裝方法，完整說明可以參閱官方文件。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip install SpeechRecognition</span><br></pre></td></tr></table></figure>

<p>接下來只需要在 python 中導入模組即可。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> speech_recognition <span class="keyword">as</span> sr</span><br></pre></td></tr></table></figure>

<p>先初始化麥克風（建議放在程式碼開頭）接著我們將音訊輸入源調成麥克風，這裡會用 Google 的 Speech to Text 服務，將語音轉成文字，並存入 message 這個變數裡面。這裡會用 try, except 來處理無法辨識語音及連線到 Google 伺服器的錯誤（這邊我沒有將 Google 伺服器回彈的錯誤碼輸出，可以自己更改程式，或是直接在終端機查看）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># initialize the recognizer</span></span><br><span class="line">r = sr.Recognizer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># use the microphone as the source for input</span></span><br><span class="line"><span class="keyword">with</span> sr.Microphone() <span class="keyword">as</span> source:                             <span class="comment"># 設定麥克風為音訊輸入源</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Adjusting for ambient noise, please wait...&quot;</span>)</span><br><span class="line">    r.adjust_for_ambient_noise(source)                      <span class="comment"># 處理背景雜音</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Say something!&quot;</span>)</span><br><span class="line">    audio = r.listen(source)                                <span class="comment"># 開始收音</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        message = r.recognize_google(audio)                 <span class="comment"># 將語音轉成文字</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;ME: &quot;</span>, r.recognize_google(audio))</span><br><span class="line">    <span class="keyword">except</span> sr.UnknownValueError:                            <span class="comment"># 語音無法辨識</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Google Web Speech could not understand the audio&quot;</span>)</span><br><span class="line">    <span class="keyword">except</span> sr.Request <span class="keyword">as</span> e:                                 <span class="comment"># google伺服器錯誤訊息</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Could not request results form Google Web service&quot;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="Brain"><a href="#Brain" class="headerlink" title="Brain"></a>Brain</h2><p>確認已安裝 Hugging Chat API。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip install hugchat</span><br></pre></td></tr></table></figure>

<p>接著將 Hugging Chat 導入到程式碼。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> hugchat <span class="keyword">import</span> hugchat</span><br><span class="line"><span class="keyword">from</span> hugchat.login <span class="keyword">import</span> Login</span><br></pre></td></tr></table></figure>

<p>導入完後，確認已申請 Huggingface 的帳號，並將 email, passwd 分別輸入申請時的電子郵件帳號及 Huggingface 的密碼，這是用來在 python 程式中登入 Huggingface 帳號來使用 Hugging Chat的功能。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># login</span></span><br><span class="line">email = <span class="string">&quot;&quot;</span>                                                  <span class="comment"># 請輸入自己的email</span></span><br><span class="line">passwd = <span class="string">&quot;&quot;</span>                                                 <span class="comment"># 請輸入自己的密碼</span></span><br><span class="line">sign = Login(email, passwd)</span><br><span class="line">cookies = sign.login()</span><br></pre></td></tr></table></figure>

<p>登入完後就可以啟動它了，這邊所使用 jarvis 變數可以改成任何名字。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># activate</span></span><br><span class="line">jarvis = hugchat.ChatBot(cookies=cookies.get_dict())</span><br></pre></td></tr></table></figure>

<p>這邊建議將 login 和 activate 這兩個片段放在程式碼前端，完整程式碼我會放在最尾端。<br>接著我們在語音辨識的程式碼後面將辨識好的文字送給 Hugging Chat 生成回答。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">response = jarvis.chat(message)</span><br></pre></td></tr></table></figure>

<h2 id="語音合成"><a href="#語音合成" class="headerlink" title="語音合成"></a>語音合成</h2><p>終於來到語音合成。這裡使用的語音合成模型會是 Google 的 gTTS，語音合成模型很多，也可以使用pytts 或是 OpenAI 的whisperAPI（這要付費，有一個開源模型也是whisper，是 OpenAI 訓練的，但他只有與語音轉文字而已，在 Gama 版我會介紹並使用）。除了 gTTS 我還會再使用 pydub 方便合成處理及播放音訊。<br>安裝 <a href="https://pypi.org/project/gTTS/">gTTS</a> 及 <a href="https://pypi.org/project/pydub/">pydub</a>可以參閱說明文件。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip install gTTS pydub</span><br></pre></td></tr></table></figure>

<p>接著只要將 gTTS 導入 python 程式就可以了。這裡我們要使用 pydub 裡的 AudioSegment 及 play 而已，以此只導入這兩個函式。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> gtts <span class="keyword">import</span> gTTS</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pydub <span class="keyword">import</span> AudioSegment</span><br><span class="line"><span class="keyword">from</span> pydub.playback <span class="keyword">import</span> play</span><br></pre></td></tr></table></figure>

<p>導入完工具後，接下來進行語音合成。設定合成的語言是英文，再把檔案存成 mp3 格式，但因為在之後用 pygame 編寫 GUI (使用者介面）時，只支援 16 位的 PCM wav 檔，因此要做轉檔的動作，有關 GUI 介面我會再找時間發一篇文章。 </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">speak</span>(<span class="params">text</span>):</span><br><span class="line">    tts = gTTS(text=text, lang=<span class="string">&#x27;en&#x27;</span>) <span class="comment"># 合成語言設定</span></span><br><span class="line">    tts.save(<span class="string">&quot;response.mp3&quot;</span>)         <span class="comment"># 音檔為 mp3 </span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 轉換為 16 位 PCM 格式的 WAV 文件</span></span><br><span class="line">    audio = AudioSegment.from_mp3(<span class="string">&quot;response.mp3&quot;</span>)</span><br><span class="line">    audio = audio.set_frame_rate(<span class="number">44100</span>).set_channels(<span class="number">2</span>).set_sample_width(<span class="number">2</span>)</span><br><span class="line">    audio.export(<span class="string">&quot;response.wav&quot;</span>, <span class="built_in">format</span>=<span class="string">&#x27;wav&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>編寫完函式，只要將 Hugging Chat 產生的回答放入 speak 函式中就可以生成音檔了。最後在用play 播放音檔，我會習慣在語音播放完後輸出文字，確認講的內容，這行可以自行刪減。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">speak(<span class="built_in">str</span>(response))           <span class="comment"># 強制轉型 string 格式怕 response 產生的文字格式不是字串</span></span><br><span class="line">play(AudioSegment.from_wav(<span class="string">&quot;response.wav&quot;</span>))</span><br><span class="line"><span class="built_in">print</span>(response)</span><br></pre></td></tr></table></figure>

<h1 id="完整的程式碼"><a href="#完整的程式碼" class="headerlink" title="完整的程式碼"></a>完整的程式碼</h1><p>我在這裡加上了迴圈及停止方法（按下 Ctrl + C）可以終止程式。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> speech_recognition <span class="keyword">as</span> sr</span><br><span class="line"><span class="keyword">from</span> hugchat <span class="keyword">import</span> hugchat</span><br><span class="line"><span class="keyword">from</span> hugchat.login <span class="keyword">import</span> Login</span><br><span class="line"><span class="keyword">from</span> gtts <span class="keyword">import</span> gTTS</span><br><span class="line"><span class="keyword">from</span> pydub <span class="keyword">import</span> AudioSegment</span><br><span class="line"></span><br><span class="line"><span class="comment"># speaking</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">speak</span>(<span class="params">text</span>):</span><br><span class="line">    tts = gTTS(text=text, lang=<span class="string">&#x27;en&#x27;</span>)</span><br><span class="line">    tts.save(<span class="string">&quot;response.mp3&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 轉換為 16 位 PCM 格式的 WAV 文件</span></span><br><span class="line">    audio = AudioSegment.from_mp3(<span class="string">&quot;response.mp3&quot;</span>)</span><br><span class="line">    audio = audio.set_frame_rate(<span class="number">44100</span>).set_channels(<span class="number">2</span>).set_sample_width(<span class="number">2</span>)</span><br><span class="line">    audio.export(<span class="string">&quot;response.wav&quot;</span>, <span class="built_in">format</span>=<span class="string">&#x27;wav&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># initialize the recognizer</span></span><br><span class="line">r = sr.Recognizer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># log in</span></span><br><span class="line">email = <span class="string">&quot;&quot;</span></span><br><span class="line">passwd = <span class="string">&quot;&quot;</span></span><br><span class="line">sign = Login(email, passwd)</span><br><span class="line">cookies = sign.login()</span><br><span class="line"></span><br><span class="line"><span class="comment"># activate bot</span></span><br><span class="line">jarvis = hugchat.ChatBot(cookies=cookies.get_dict())</span><br><span class="line"></span><br><span class="line"><span class="comment"># generate response</span></span><br><span class="line"><span class="keyword">while</span>(<span class="literal">True</span>):</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="comment"># use the microphone as the source for input</span></span><br><span class="line">        <span class="keyword">with</span> sr.Microphone() <span class="keyword">as</span> source:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Adjusting for ambient noise, please wait...&quot;</span>)</span><br><span class="line">            r.adjust_for_ambient_noise(source)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Say something!&quot;</span>)</span><br><span class="line">            audio = r.listen(source)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            message = r.recognize_google(audio)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;ME: &quot;</span>, r.recognize_google(audio))</span><br><span class="line">        <span class="keyword">except</span> sr.UnknownValueError:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Google Web Speech could not understand the audio&quot;</span>)</span><br><span class="line">        <span class="keyword">except</span> sr.Request <span class="keyword">as</span> e:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Could not request results form Google Web service&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># generate the response</span></span><br><span class="line">        response = jarvis.chat(message)</span><br><span class="line">        speak(<span class="built_in">str</span>(response))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Jarvis: &quot;</span>, response)</span><br><span class="line">    <span class="keyword">except</span> KeyboardInterrupt:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Exiting...&quot;</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<h1 id="後記"><a href="#後記" class="headerlink" title="後記"></a>後記</h1><p>Github 上的檔案可能會晚點上，下一篇我會寫 Jarvis Beta 版，直接將預訓練好的模型載到電腦裡，越後面的版本呈現的效果越酷炫。</p>
]]></content>
      <tags>
        <tag>Jarvis</tag>
        <tag>Huggingface</tag>
        <tag>gTTS</tag>
        <tag>AI</tag>
      </tags>
  </entry>
  <entry>
    <title>Ollama</title>
    <url>/2024/09/23/Ollama/</url>
    <content><![CDATA[<p>前言</p>
<p>這次要介紹一個好用的軟體能讓 AI 模型在本機上運行，不用擔心數據被拿去當作訓練資料&lt;–!more–&gt;</p>
<h2 id="何謂-Ollama"><a href="#何謂-Ollama" class="headerlink" title="何謂 Ollama"></a>何謂 Ollama</h2><p><a href="https://ollama.com/">Ollama</a> 是一個開源軟體，能夠將 LLM（大型語言模型）在本地端運行，而且可以不需要使用 GPU。當然，要使用 GPU來加速運算也是沒問題的。他提供許多開源的語言模型如：Llama 3.1, Mistral，當然除了大型的模型以外，也有一些小型的模型如：Phi-3，可以用於 RAG 的訓練，之後會再提一些有關 RAG 的東西。</p>
<h2 id="如何使用-Ollama"><a href="#如何使用-Ollama" class="headerlink" title="如何使用 Ollama"></a>如何使用 Ollama</h2><p>最簡單的就是進入 <a href="https://ollama.com/">Ollama</a> 官網，點下 Download，然後照著他的指示安裝，或著你是用 Mac OS 的可以用 hombrew 來安裝。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">brew install ollama</span><br></pre></td></tr></table></figure>

<p>若使用 Linux 系統的除了從官網下載，也可以使用以下命令。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl https://ollama.ai/install.sh | sh</span><br></pre></td></tr></table></figure>

<p>安裝好以後，可以到官網尋找喜歡的模型，複製貼上網站上的 run 指令，或是已有想運行的模型，則可以參照以下指令，這裡會用 llama3.1 8b 做示範。</p>
<p>這邊會先下載 llama3.1 8b 的模型，可能有些久，因為模型檔案有點大。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ollama pull llama3.1:8b</span><br></pre></td></tr></table></figure>

<p>接著可以使用 run 指令來運行 llama3.1 8b。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ollama run llama3.1:8b</span><br></pre></td></tr></table></figure>

<p>如果想看你下載了哪些模型，可以使用以下命令。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ollama list</span><br></pre></td></tr></table></figure>

<h2 id="後記"><a href="#後記" class="headerlink" title="後記"></a>後記</h2><p>運行之後就可以把它當 ChatGPT 或是其他的 AI 聊天軟體使用了，下一章會將 Ollama 結合 Openweb UI，可以讓 AI 讀取電腦本機檔案，雖然這跟 ChatGPT 類似，但 Ollama + Openweb UI 是讓模型完全在本地運行，不用擔心資料外洩，而且 Ollama 和 Openweb UI 是免費的。</p>
]]></content>
  </entry>
  <entry>
    <title>Welcome to Bakersite</title>
    <url>/2024/08/14/Welcome-to-Bakersite/</url>
    <content><![CDATA[<p>這裡會分享一些科技相關的文章，可能有些人會因為不會寫程式而放棄去探索一些領域的內容，然而生成式AI其實可以解決程式能力的問題，希望這裡的內容，能幫助到對科技有興趣的人。文章中的程式碼大多數是由AI進行編寫，這裡的程式碼不是重點，最主要的目的是介紹一些好用得工具或是App。我會盡量將文章寫的淺顯易懂，讓更多人能體驗。</p>
]]></content>
  </entry>
  <entry>
    <title>Jarvis Beta</title>
    <url>/2024/08/19/Jarvis-Beta/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>上一篇有提到 Huggingface 這次要來使用它的 pipeline 取用模型了。其實用 Hugging Chat 有一個弊端是伺服器運作不穩定，常常莫名其妙斷線，這問題不論是使用 Hugging Chat API 還是 Hugging Chat 自己的介面都有出現。<span id="more"></span>因此在編寫這個版本時，我使以改良 Jarvis 的大腦運行回主要目標。一開始曾想用 Huggingface 上的生成式模型，但因為硬體限制，所以只能使用 Inference API 在 Huggingface 伺服器運算完之後再傳回本機。其他的 STT(Speech to Text) 及 TTS(Text to Speech)模型還在硬體負荷範圍，因此我把 Google 的 TTS 換掉，畢竟我實在無法接受 Google 小姐的聲音，而 TTS 是因為我想加上喚醒詞(Jarvis)，因此順便把 TTS 也換掉。Beta 版<a href="https://huggingface.co/learn/audio-course/chapter7/voice-assistant">參考此網站</a>，這裡會寫的比較精簡，有興趣的可以進網站看看。</p>
<h1 id="Huggingface"><a href="#Huggingface" class="headerlink" title="Huggingface"></a>Huggingface</h1><p><a href="https://huggingface.co/">Hugginface</a>，如同上一篇講到的，他是一個開源的模型平台，可以從上面下載到很多 AI 模型。在 Beta 版本中，我會下載 <a href="https://huggingface.co/MIT/ast-finetuned-speech-commands-v2">MIT&#x2F;ast-finetuned-speech-commands-v2</a>模型用來識別喚醒詞。比較可惜的是在訓練的模型裡沒有 Jarvis，這裡我選擇用 Marvin 來替代。然後會使用 <a href="https://huggingface.co/openai/whisper-base.en">whisper</a> 進行文字轉語音工作。生成語音則是下載 <a href="https://huggingface.co/microsoft/speecht5_tts">microsoft&#x2F;speecht5_tts</a>。至於 Beta 的大腦我會使用 Huggingface 特有的 Inference API 服務，這裡我選擇的是<a href="https://huggingface.co/tiiuae/falcon-7b-instruct">falcon7b</a>當作大腦生成回答。</p>
<h1 id="實作"><a href="#實作" class="headerlink" title="實作"></a>實作</h1><pre class="mermaid">flowchart LR
A(Whisper-STT)
B[(Falcon7b-Inference API)]
C(SpeechT5-TTS)
A --> B --> C --> A</pre>

<h1 id="語音辨識-STT"><a href="#語音辨識-STT" class="headerlink" title="語音辨識(STT)"></a>語音辨識(STT)</h1><p>Whisper 的語音辨識是由 Open AI 訓練的開源模型，如果經濟許可的話它也有付費的 API，但這樣的話可以考慮直接用 Chat GPT。</p>
<p>首先要先導入pipeline，然後在這裡設定要用什麼運行，這邊是如果有 Nvidia的顯卡優先用，如果沒有就使用電腦的cpu。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> pipeline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">device = <span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br></pre></td></tr></table></figure>

<p>在這裡設定使用 whisper 語音辨識模型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">transcriber = pipeline(</span><br><span class="line">    <span class="string">&quot;automatic-speech-recognition&quot;</span>, model=<span class="string">&quot;openai/whisper-base.en&quot;</span>, device=device</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>這邊是編寫轉換的函式。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">transcribe</span>(<span class="params">chunk_length_s=<span class="number">5.0</span>, stream_chunk_s=<span class="number">1.0</span></span>):</span><br><span class="line">    sampling_rate = transcriber.feature_extractor.sampling_rate</span><br><span class="line"></span><br><span class="line">    mic = ffmpeg_microphone_live(</span><br><span class="line">        sampling_rate=sampling_rate,</span><br><span class="line">        chunk_length_s=chunk_length_s,</span><br><span class="line">        stream_chunk_s=stream_chunk_s,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Start speaking...&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> transcriber(mic, generate_kwargs=&#123;<span class="string">&quot;max_new_tokens&quot;</span>: <span class="number">128</span>&#125;):</span><br><span class="line">        sys.stdout.write(<span class="string">&quot;\033[K&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(item[<span class="string">&quot;text&quot;</span>], end=<span class="string">&quot;\r&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> item[<span class="string">&quot;partial&quot;</span>][<span class="number">0</span>]:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> item[<span class="string">&quot;text&quot;</span>]</span><br></pre></td></tr></table></figure>

<p>使用 Huggingface 的 Inference API。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> huggingface_hub <span class="keyword">import</span> HfFolder</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">query</span>(<span class="params">text, model_id=<span class="string">&quot;tiiuae/falcon-7b-instruct&quot;</span></span>):</span><br><span class="line">    api_url = <span class="string">f&quot;https://api-inference.huggingface.co/models/<span class="subst">&#123;model_id&#125;</span>&quot;</span></span><br><span class="line">    headers = &#123;<span class="string">&quot;Authorization&quot;</span>: <span class="string">f&quot;Bearer <span class="subst">&#123;HfFolder().get_token()&#125;</span>&quot;</span>&#125;</span><br><span class="line">    payload = &#123;<span class="string">&quot;inputs&quot;</span>: text&#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Querying...: <span class="subst">&#123;text&#125;</span>&quot;</span>)</span><br><span class="line">    response = requests.post(api_url, headers=headers, json=payload)</span><br><span class="line">    <span class="keyword">return</span> response.json()[<span class="number">0</span>][<span class="string">&quot;generated_text&quot;</span>][<span class="built_in">len</span>(text) + <span class="number">1</span> :]</span><br></pre></td></tr></table></figure>

<p>使用 SpeechT5 的 TTS 模型</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan</span><br><span class="line"></span><br><span class="line">processor = SpeechT5Processor.from_pretrained(<span class="string">&quot;microsoft/speecht5_tts&quot;</span>)</span><br><span class="line"></span><br><span class="line">model = SpeechT5ForTextToSpeech.from_pretrained(<span class="string">&quot;microsoft/speecht5_tts&quot;</span>).to(device)</span><br><span class="line">vocoder = SpeechT5HifiGan.from_pretrained(<span class="string">&quot;microsoft/speecht5_hifigan&quot;</span>).to(device)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line">embeddings_dataset = load_dataset(<span class="string">&quot;Matthijs/cmu-arctic-xvectors&quot;</span>, split=<span class="string">&quot;validation&quot;</span>)</span><br><span class="line">speaker_embeddings = torch.tensor(embeddings_dataset[<span class="number">7306</span>][<span class="string">&quot;xvector&quot;</span>]).unsqueeze(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">synthesise</span>(<span class="params">text</span>):</span><br><span class="line">    inputs = processor(text=text, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">    speech = model.generate_speech(</span><br><span class="line">        inputs[<span class="string">&quot;input_ids&quot;</span>].to(device), speaker_embeddings.to(device), vocoder=vocoder</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> speech.cpu()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> Audio</span><br><span class="line"></span><br><span class="line">audio = synthesise(</span><br><span class="line">    <span class="string">&quot;Hugging Face is a company that provides natural language processing and machine learning tools for developers.&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">Audio(audio, rate=<span class="number">16000</span>)</span><br></pre></td></tr></table></figure>

<p>最後是主程式的部分</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">launch_fn()</span><br><span class="line">transcription = transcribe()</span><br><span class="line">response = query(transcription)</span><br><span class="line">audio = synthesise(response)</span><br><span class="line"></span><br><span class="line">Audio(audio, rate=<span class="number">16000</span>, autoplay=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h1 id="後記"><a href="#後記" class="headerlink" title="後記"></a>後記</h1><p>基本上這是參考 <a href="https://huggingface.co/learn/audio-course/chapter7/voice-assistant#speech-transcription">Huggingface voice assistant</a> 網站的，所以這邊只加了一些說明而已，有興趣的可以直接參考網站的製作，但這 Beta 版在我測試時有個 Bug，透過 Inference API 所產生的回答，會有點跟問題對不上，有點胡言亂語。</p>
]]></content>
      <tags>
        <tag>Jarvis</tag>
        <tag>Huggingface</tag>
        <tag>AI</tag>
      </tags>
  </entry>
</search>
